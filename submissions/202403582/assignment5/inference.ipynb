{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 드라이브 연동"
      ],
      "metadata": {
        "id": "J_IJ_rHZxKZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "%cd /content/drive/MyDrive/ML_Project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFOEddH0wtGF",
        "outputId": "e480e5e3-1634-4bb4-842d-c37e91cf3a61"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/ML_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 및 벡터 로드"
      ],
      "metadata": {
        "id": "Mr1HTajXxMCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Logistic Regression 최종 모델 로드\n",
        "model = joblib.load(\"final_model.pkl\")\n",
        "tfidf = joblib.load(\"tfidf_vectorizer.pkl\")\n",
        "\n",
        "print(\"모델 및 벡터 로드 완료\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lmaRXTdw2RE",
        "outputId": "88738b8a-7293-4014-eb12-6b75e3beda3e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델 및 벡터 로드 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전처리(기존과 동일)"
      ],
      "metadata": {
        "id": "ePL1ZE7FxPKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Training 때 사용한 stopword 리스트 그대로 복붙\n",
        "stopwords = set([\n",
        "    # 1) filler / 말버릇\n",
        "    \"어\", \"음\", \"자\", \"뭐\", \"요\", \"네\", \"막\", \"그냥\",\n",
        "    \"근데\", \"그죠\", \"거죠\", \"예\",\n",
        "\n",
        "    # 2) 지시어 (정보 없음)\n",
        "    \"이거\", \"이게\", \"이건\", \"그거\", \"그게\",\n",
        "    \"저거\", \"요거\", \"요게\", \"얘는\",\n",
        "\n",
        "    # 3) 공손/형식적 표현\n",
        "    \"합니다\", \"되었습니다\", \"됩니다\", \"하겠습니다\",\n",
        "    \"해주세요\", \"드릴\", \"보겠습니다\", \"할게요\",\n",
        "    \"있습니다\", \"있어요\", \"있죠\",\n",
        "\n",
        "    # 4) 기능어 (기본 조사)\n",
        "    \"이\", \"그\", \"저\", \"을\", \"를\", \"은\", \"는\", \"에\",\n",
        "    \"에서\", \"로\", \"것\", \"거\", \"건\", \"것들\",\n",
        "])\n",
        "\n",
        "def preprocess(text):\n",
        "    text = str(text).lower()                      # 소문자 변환\n",
        "    text = re.sub(r\"[^가-힣a-zA-Z0-9\\s]\", \" \", text)  # 한글/영문/숫자 제외 모두 제거\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()      # 중복 공백 제거\n",
        "\n",
        "    # 토큰화\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Stopword 제거\n",
        "    tokens = [t for t in tokens if t not in stopwords]\n",
        "\n",
        "    # Training과 완전히 동일하게 문장 재구성\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "Mn2TOlZ9w3mN"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 단일 문장 추론"
      ],
      "metadata": {
        "id": "SIjv8R0qxR6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentence(sentence, threshold=0.06):\n",
        "    clean = preprocess(sentence)\n",
        "    vec = tfidf.transform([clean])\n",
        "    prob_1 = model.predict_proba(vec)[0][1]\n",
        "    pred = 1 if prob_1 >= threshold else 0\n",
        "    return pred, prob_1"
      ],
      "metadata": {
        "id": "t3t-tIN8w6LE"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 여러 문장 리스트 추론"
      ],
      "metadata": {
        "id": "_QaSc0EgxTz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_batch(sentences, threshold=0.06):\n",
        "    clean_list = [preprocess(s) for s in sentences]\n",
        "    vec = tfidf.transform(clean_list)\n",
        "    prob_1 = model.predict_proba(vec)[:, 1]\n",
        "    preds = (prob_1 >= threshold).astype(int)\n",
        "    return preds, prob_1"
      ],
      "metadata": {
        "id": "uNUbZoJxw7_v"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 테스트"
      ],
      "metadata": {
        "id": "W5wy7LkExXTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_sentences = [\n",
        "    \"이번 알고리즘의 핵심 아이디어는 데이터를 반복적으로 갱신하는 것입니다.\",\n",
        "    \"이 모델의 목적은 입력 문장에서 핵심 정보를 자동으로 추출하는 것입니다.\",\n",
        "    \"여기까지 첫 번째 파트 설명이었습니다.\",\n",
        "    \"혹시 질문 있으신가요?\"\n",
        "]\n",
        "\n",
        "preds, probs = predict_batch(example_sentences, threshold=0.06)\n",
        "\n",
        "for s, p, pr in zip(example_sentences, preds, probs):\n",
        "    print(f\"[문장] {s}\")\n",
        "    print(f\"=> 예측 라벨: {p} (확률: {pr:.4f})\")\n",
        "    print(\"   • 1 = 핵심 문장 / 0 = 비핵심 문장\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xs8NYeKPw_Ad",
        "outputId": "d0a975eb-37c2-4dab-f900-5c18f3ea9844"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[문장] 이번 알고리즘의 핵심 아이디어는 데이터를 반복적으로 갱신하는 것입니다.\n",
            "=> 예측 라벨: 1 (확률: 0.1262)\n",
            "   • 1 = 핵심 문장 / 0 = 비핵심 문장\n",
            "\n",
            "[문장] 이 모델의 목적은 입력 문장에서 핵심 정보를 자동으로 추출하는 것입니다.\n",
            "=> 예측 라벨: 1 (확률: 0.0617)\n",
            "   • 1 = 핵심 문장 / 0 = 비핵심 문장\n",
            "\n",
            "[문장] 여기까지 첫 번째 파트 설명이었습니다.\n",
            "=> 예측 라벨: 0 (확률: 0.0329)\n",
            "   • 1 = 핵심 문장 / 0 = 비핵심 문장\n",
            "\n",
            "[문장] 혹시 질문 있으신가요?\n",
            "=> 예측 라벨: 0 (확률: 0.0080)\n",
            "   • 1 = 핵심 문장 / 0 = 비핵심 문장\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    s = input(\"문장을 입력하세요 (종료: quit): \")\n",
        "    if s.lower() == \"quit\":\n",
        "        break\n",
        "    pred, prob = predict_sentence(s)\n",
        "    print(f\"예측 라벨: {pred} (확률: {prob:.4f})  /  1=핵심, 0=비핵심\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqKiAoN0xA5B",
        "outputId": "b510e131-60a0-466c-e74f-1342b67d694b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문장을 입력하세요 (종료: quit): 분할 정복(Divide and Conquer) 알고리즘은 하나의 큰 문제를 작은 하위 문제로 분할(Divide)하고, 각 하위 문제를 해결(Conquer)한 뒤, 그 해결책들을 합쳐서(Combine) 원래의 문제를 해결하는 알고리즘 설계 기법입니다.\n",
            "예측 라벨: 1 (확률: 0.2749)  /  1=핵심, 0=비핵심\n",
            "\n",
            "문장을 입력하세요 (종료: quit): 여러분들 전부 이해하셨죠?\n",
            "예측 라벨: 0 (확률: 0.0502)  /  1=핵심, 0=비핵심\n",
            "\n",
            "문장을 입력하세요 (종료: quit): 아 네 그러면 여기까지 하겠습니다.\n",
            "예측 라벨: 0 (확률: 0.0254)  /  1=핵심, 0=비핵심\n",
            "\n",
            "문장을 입력하세요 (종료: quit): 잠시만요, 화면이 안 보이네요.\n",
            "예측 라벨: 0 (확률: 0.0081)  /  1=핵심, 0=비핵심\n",
            "\n",
            "문장을 입력하세요 (종료: quit): quit\n"
          ]
        }
      ]
    }
  ]
}